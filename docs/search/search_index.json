{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"What is Active Tigger ? ActiveTigger 1 is an text annotation web tool dedicated for computational social sciences. It is designed to assist exploration and model ( BERT ) fine-tuning to annotate text dataset relying on active learning. How to start ? First, you need to access a running instance : Deploy you local instance (with a GPU if you want shorter computation time, but it can run on CPU) locally on a cloud server (for instance OVH ) Get access to existing instance The current version is a refactor of R Shiny ActiveTigger app (Julien Boelaert & Etienne Ollion). Active Tigger name is a pun that draws on the similarity between the words 'Tagger' and 'Tigger.' \u21a9","title":"Home"},{"location":"#what-is-active-tigger","text":"ActiveTigger 1 is an text annotation web tool dedicated for computational social sciences. It is designed to assist exploration and model ( BERT ) fine-tuning to annotate text dataset relying on active learning.","title":"What is Active Tigger ?"},{"location":"#how-to-start","text":"First, you need to access a running instance : Deploy you local instance (with a GPU if you want shorter computation time, but it can run on CPU) locally on a cloud server (for instance OVH ) Get access to existing instance The current version is a refactor of R Shiny ActiveTigger app (Julien Boelaert & Etienne Ollion). Active Tigger name is a pun that draws on the similarity between the words 'Tagger' and 'Tigger.' \u21a9","title":"How to start ?"},{"location":"access/","text":"Get access An instance of Active Tigger is running on GENES servers with a GPU. The beta test phase started October 30, 2024 and will last until August 2025. The main aim is to report bugs and problems The secondary aim is to identify potential enhancements [!NOTE] During this phase, be informed that the service can evolve since bug fixes will be integrated continuously. An email will be sent before any reset that could led to destroyed data. Nevetheless, please take care to save your data. Who can access the service Researchers who have textual data to annotate Open source contributors who want to help on a scientific software All accounts will be reset at the end of the beta test phase. Account without any activity can be deactivated after 2 months. Please be aware that this is a test service, and we reserve the right to deactivate any account at our discretion. How to test Contact emilien.schultz [at] ensae.fr to request access. Please include a brief explanation of: Your current status (e.g., student, researcher) Your research project or use case Once access is granted, you can: Upload data Annotate text Train models Report bugs or problems via GitHub issues How to Discuss Use GitHub issues for bug reports and feature suggestions. Join our Discord channel (request an invitation via email). User agreement \u26a0\ufe0f Access to Active Tigger is currently temporary , and data storage or security is not guaranteed . The service is hosted on ENSAE servers and intended solely for research purposes . We reserve the right to modify, restrict, or revoke access at any time without notice. To get started Request account (email above) Introduce yourself and your project on the Discord server Check out the Quick Start guide","title":"Get Access"},{"location":"access/#get-access","text":"An instance of Active Tigger is running on GENES servers with a GPU. The beta test phase started October 30, 2024 and will last until August 2025. The main aim is to report bugs and problems The secondary aim is to identify potential enhancements [!NOTE] During this phase, be informed that the service can evolve since bug fixes will be integrated continuously. An email will be sent before any reset that could led to destroyed data. Nevetheless, please take care to save your data.","title":"Get access"},{"location":"access/#who-can-access-the-service","text":"Researchers who have textual data to annotate Open source contributors who want to help on a scientific software All accounts will be reset at the end of the beta test phase. Account without any activity can be deactivated after 2 months. Please be aware that this is a test service, and we reserve the right to deactivate any account at our discretion.","title":"Who can access the service"},{"location":"access/#how-to-test","text":"Contact emilien.schultz [at] ensae.fr to request access. Please include a brief explanation of: Your current status (e.g., student, researcher) Your research project or use case Once access is granted, you can: Upload data Annotate text Train models Report bugs or problems via GitHub issues","title":"How to test"},{"location":"access/#how-to-discuss","text":"Use GitHub issues for bug reports and feature suggestions. Join our Discord channel (request an invitation via email).","title":"How to Discuss"},{"location":"access/#user-agreement","text":"\u26a0\ufe0f Access to Active Tigger is currently temporary , and data storage or security is not guaranteed . The service is hosted on ENSAE servers and intended solely for research purposes . We reserve the right to modify, restrict, or revoke access at any time without notice.","title":"User agreement"},{"location":"access/#to-get-started","text":"Request account (email above) Introduce yourself and your project on the Discord server Check out the Quick Start guide","title":"To get started"},{"location":"contributors/","text":"Contributors Scientific board \u00c9tienne Ollion (CREST) Julien Boelaert (Universit\u00e9 de Lille) Emma Bonutti (CREST) Annina Claesson (CREST) Arnault Chatelain (CREST) \u00c9milien Schultz (CREST) Contributors \u00c9milien Schultz (CREST) Paul Girard (Ouestware) Jean-Baptiste Richardet Matthias Bussonnier L\u00e9o Mignot (Centre \u00c9mile Durkheim) Jules Brion (Pacte) Annina Claesson (CREST) Feedbacks David Larousserie Funding DRARI \u00cele-de-France CREST @ IPP Prog\u00e9do How to contribute If you want to contribute to the codebase, please get in touch.","title":"Contributors"},{"location":"contributors/#contributors","text":"","title":"Contributors"},{"location":"contributors/#scientific-board","text":"\u00c9tienne Ollion (CREST) Julien Boelaert (Universit\u00e9 de Lille) Emma Bonutti (CREST) Annina Claesson (CREST) Arnault Chatelain (CREST) \u00c9milien Schultz (CREST)","title":"Scientific board"},{"location":"contributors/#contributors_1","text":"\u00c9milien Schultz (CREST) Paul Girard (Ouestware) Jean-Baptiste Richardet Matthias Bussonnier L\u00e9o Mignot (Centre \u00c9mile Durkheim) Jules Brion (Pacte) Annina Claesson (CREST)","title":"Contributors"},{"location":"contributors/#feedbacks","text":"David Larousserie","title":"Feedbacks"},{"location":"contributors/#funding","text":"DRARI \u00cele-de-France CREST @ IPP Prog\u00e9do","title":"Funding"},{"location":"contributors/#how-to-contribute","text":"If you want to contribute to the codebase, please get in touch.","title":"How to contribute"},{"location":"documentation/","text":"Documentation This documentation details the different logics and functionalities of Active Tigger. It is a work in progress. It will be updated as the project evolves. General comments Only one process allowed in the same time by user. There is two types of processes in Active Tigger: CPU processes: they are used to prepare data, train quick models, etc. GPU processes: these processes are run on the GPU. The server can only run a limited number of GPU processes at the same time. In both case, if the number of processes is too high, they will be queued. Accounts A user can have the following status : root : can create and manage projects, users, and all data. Can access all projects and the /monitor page. manager : can create and manage projects where he.she has complete rights. Can add users to the projects. annotator : can only annotate on projects where he.she have been added. The frontend is simplified for this role. User have also a relational status for a specific project: manager : can manage the project, add users, and access all data. contributor : can create elements in a project but never delete things annotator : can only annotate on the project without changing the project settings (schemes, labels, models, etc.). For the moment, there is no management at the scheme level. Create a project Prepare labels and features Explore the data Annotate Fine-tune a BERT model Test model The test set: - Created on the beginning of the project - Uploaded latter Once activated, the test mode : - Deactivate for the user the choice of scheme, label management - Allow only annotation for the test set - Allow to explore the test set Export data / models","title":"User Guide"},{"location":"documentation/#documentation","text":"This documentation details the different logics and functionalities of Active Tigger. It is a work in progress. It will be updated as the project evolves.","title":"Documentation"},{"location":"documentation/#general-comments","text":"Only one process allowed in the same time by user. There is two types of processes in Active Tigger: CPU processes: they are used to prepare data, train quick models, etc. GPU processes: these processes are run on the GPU. The server can only run a limited number of GPU processes at the same time. In both case, if the number of processes is too high, they will be queued.","title":"General comments"},{"location":"documentation/#accounts","text":"A user can have the following status : root : can create and manage projects, users, and all data. Can access all projects and the /monitor page. manager : can create and manage projects where he.she has complete rights. Can add users to the projects. annotator : can only annotate on projects where he.she have been added. The frontend is simplified for this role. User have also a relational status for a specific project: manager : can manage the project, add users, and access all data. contributor : can create elements in a project but never delete things annotator : can only annotate on the project without changing the project settings (schemes, labels, models, etc.). For the moment, there is no management at the scheme level.","title":"Accounts"},{"location":"documentation/#create-a-project","text":"","title":"Create a project"},{"location":"documentation/#prepare-labels-and-features","text":"","title":"Prepare labels and features"},{"location":"documentation/#explore-the-data","text":"","title":"Explore the data"},{"location":"documentation/#annotate","text":"","title":"Annotate"},{"location":"documentation/#fine-tune-a-bert-model","text":"","title":"Fine-tune a BERT model"},{"location":"documentation/#test-model","text":"The test set: - Created on the beginning of the project - Uploaded latter Once activated, the test mode : - Deactivate for the user the choice of scheme, label management - Allow only annotation for the test set - Allow to explore the test set","title":"Test model"},{"location":"documentation/#export-data-models","text":"","title":"Export data / models"},{"location":"environment/","text":"What is the environmental impact of the models used on Active Tigger? Using AI models come with a consumption of energy and resources that are important to consider. This page discusses the environmental impact of models. The environmental impact of AI models Environmental impacts of AI raise important concerns and became an important issue for research with methodologies to mesure it . For NLP, the energy consumption can varie greatly depending on the type of technology used , the energy origin used to run the servers but also on the the methodology chosen to evaluate the impacts. Active Tigger is designed to take benefits of small models with reduced energy dependency (compared to genAI). It uses BERT models, which are small, and then less energy-intensive than large generative models . However, it is still important to understand the environmental impact of these models and the methodologies used to evaluate them. Reminder : a flight between Paris and New York emits about 2 tonnes of CO\u2082 per passenger . When does a model consume energy ? There are 3 main phases in the life cycle of a model that can consume energy and have an environmental impact : the training phase , the fine-tuning phase , and the inference phase . \u2699\ufe0f Training phase This phase involve using a huge text dataset to train a model. Several studies show that generative models (such as GPT) consume vast resources at this stage . An analysis of the GPT-3 model , which contains 175 billion parameters, estimated it generated 552.1 tonnes of CO\u2082. By contrast, models smaller such as BERT (~100 millions parameters) consume less energy . One research team trained several BERT models using different configurations (hardware, batch size, sequence length). The environmental cost ranged from 58.9 kg of CO\u2082 for 124.1 kWh to 199.1 kg of CO\u2082 for 419.6 kWh. Another study measured 1,438 kg of CO\u2082 for training a BERT base model on 64 V100 GPUs \u2014 showing how emissions depend heavily on the hardware used and the data volume processed . \ud83e\uddea Fine-tuning phase Fine-tuning involves adapting a pre-trained language model to a specific task. This phase is less energy-intensive than the initial training . Nevertheless, it is performed far more frequently than the initial training. One study \"Energy and Carbon Considerations of Fine-Tuning BERT\" showed that energy consumption during fine-tuning depends mainly on the total number of tokens processed (rather than their individual size) and the wall clock time. \ud83d\udd0d Inference phase Inference is the phase where the model is used to make predictions or generate text based on new inputs. It is generally less energy-intensive than training, but it can still have a significant environmental impact, especially when the model is used frequently or at scale. In the case of research, models are not scaled for millions of users, so their use is less energy-intensive than general public applications. However, the energy consumption of inference can still be significant , especially for large models or when processing large volumes of data. Going further: how to evaluate the environmental impact of AI models? The energy consumption of models To evaluate a model\u2019s impact, it is possible to examine the energy consumed by the infrastructure used during training . Result depends in particular on the country \u2014 and therefore its carbon mix \u2014 where the servers are located. For example, while training the Bloom model, using the Jean-Zay supercomputer in France, required 433 MWh of electricity (compared to 324 MWh for the OPT model), Bloom\u2019s training generated 25 tonnes of CO\u2082, versus 70 tonnes for OPT. In addition to that, the reported environmental impact depends on the type of energy reported by companies . Some companies purchase green certificates, which neither guarantee actual renewable electricity consumption nor the funding of new infrastructure . Others opt for PPA contracts, which directly finance renewable energy production through long-term agreements with producers. Evaluating the energy consumption of models is complicated. For instance, most studies do not account for all the steps preceding the final model training , such as experiments on intermediate versions, so-called \u201cablation\u201d tests (removing parts of the model to test their importance), or the many adjustments needed to reach the final version. Some authors estimate that these stages alone can double the total carbon footprint of a project. Most studies that evaluate the environmental impact of language models (LLMs) only consider part of the problem. As some authors point out , it is essential to conduct a full life cycle assessment (LCA) . This means considering not only the energy used during training, but also the pollution linked to the manufacture of the required infrastructure (servers, storage devices, etc.), from the extraction of raw materials to end-of-life disposal. This includes two often overlooked components: Embodied emissions: all the pollution generated upstream (manufacturing, transport, installation of equipment); Idle consumption: electricity used by servers even when not actively in use. As an example, the carbon footprint associated with training the Bloom model rises from 25 to nearly 50 tonnes of CO\u2082 when all these parameters are included. Furthermore, it is important not to limit impact assessments to carbon emissions alone . One must consider all forms of pollution generated. For instance, the manufacture of chips used to train and run models requires vast quantities of water \u2014 another major ecological cost that is often overlooked. To assess the environmental impact of deep learning models, one must also take into account the energy spent on fine-tuning the models and running them during inference . While hard to estimate, some studies suggest that inference (using the model) accounts for the vast majority of energy consumption in large models (especially GPT-type). A few additional remarks At first glance, using models like BERT \u2014 whether for training, fine-tuning, or inference \u2014 appears less energy-intensive than using large generative models like GPT. However, even if these models consume relatively less, this is still important to keep in mind their potential impacts , especially if the number of users increases. As we have seen, current impact assessment methodologies do not account for all pollution sources . It is therefore essential to minimize unnecessary energy use and consistently question the relevance and necessity of each project in light of the energy it consumes . Furthermore, even in countries with a largely decarbonized energy mix, electricity remains a limited resource \u2014 one that could be allocated to other equally or more essential needs. A low-carbon mix does not justify irresponsible use, nor should it lead to forgetting the principle of digital sobriety .","title":"Environment"},{"location":"environment/#what-is-the-environmental-impact-of-the-models-used-on-active-tigger","text":"Using AI models come with a consumption of energy and resources that are important to consider. This page discusses the environmental impact of models.","title":"What is the environmental impact of the models used on Active Tigger?"},{"location":"environment/#the-environmental-impact-of-ai-models","text":"Environmental impacts of AI raise important concerns and became an important issue for research with methodologies to mesure it . For NLP, the energy consumption can varie greatly depending on the type of technology used , the energy origin used to run the servers but also on the the methodology chosen to evaluate the impacts. Active Tigger is designed to take benefits of small models with reduced energy dependency (compared to genAI). It uses BERT models, which are small, and then less energy-intensive than large generative models . However, it is still important to understand the environmental impact of these models and the methodologies used to evaluate them. Reminder : a flight between Paris and New York emits about 2 tonnes of CO\u2082 per passenger .","title":"The environmental impact of AI models"},{"location":"environment/#when-does-a-model-consume-energy","text":"There are 3 main phases in the life cycle of a model that can consume energy and have an environmental impact : the training phase , the fine-tuning phase , and the inference phase .","title":"When does a model consume energy ?"},{"location":"environment/#training-phase","text":"This phase involve using a huge text dataset to train a model. Several studies show that generative models (such as GPT) consume vast resources at this stage . An analysis of the GPT-3 model , which contains 175 billion parameters, estimated it generated 552.1 tonnes of CO\u2082. By contrast, models smaller such as BERT (~100 millions parameters) consume less energy . One research team trained several BERT models using different configurations (hardware, batch size, sequence length). The environmental cost ranged from 58.9 kg of CO\u2082 for 124.1 kWh to 199.1 kg of CO\u2082 for 419.6 kWh. Another study measured 1,438 kg of CO\u2082 for training a BERT base model on 64 V100 GPUs \u2014 showing how emissions depend heavily on the hardware used and the data volume processed .","title":"\u2699\ufe0f Training phase"},{"location":"environment/#fine-tuning-phase","text":"Fine-tuning involves adapting a pre-trained language model to a specific task. This phase is less energy-intensive than the initial training . Nevertheless, it is performed far more frequently than the initial training. One study \"Energy and Carbon Considerations of Fine-Tuning BERT\" showed that energy consumption during fine-tuning depends mainly on the total number of tokens processed (rather than their individual size) and the wall clock time.","title":"\ud83e\uddea Fine-tuning phase"},{"location":"environment/#inference-phase","text":"Inference is the phase where the model is used to make predictions or generate text based on new inputs. It is generally less energy-intensive than training, but it can still have a significant environmental impact, especially when the model is used frequently or at scale. In the case of research, models are not scaled for millions of users, so their use is less energy-intensive than general public applications. However, the energy consumption of inference can still be significant , especially for large models or when processing large volumes of data.","title":"\ud83d\udd0d Inference phase"},{"location":"environment/#going-further-how-to-evaluate-the-environmental-impact-of-ai-models","text":"","title":"Going further: how to evaluate the environmental impact of AI models?"},{"location":"environment/#the-energy-consumption-of-models","text":"To evaluate a model\u2019s impact, it is possible to examine the energy consumed by the infrastructure used during training . Result depends in particular on the country \u2014 and therefore its carbon mix \u2014 where the servers are located. For example, while training the Bloom model, using the Jean-Zay supercomputer in France, required 433 MWh of electricity (compared to 324 MWh for the OPT model), Bloom\u2019s training generated 25 tonnes of CO\u2082, versus 70 tonnes for OPT. In addition to that, the reported environmental impact depends on the type of energy reported by companies . Some companies purchase green certificates, which neither guarantee actual renewable electricity consumption nor the funding of new infrastructure . Others opt for PPA contracts, which directly finance renewable energy production through long-term agreements with producers. Evaluating the energy consumption of models is complicated. For instance, most studies do not account for all the steps preceding the final model training , such as experiments on intermediate versions, so-called \u201cablation\u201d tests (removing parts of the model to test their importance), or the many adjustments needed to reach the final version. Some authors estimate that these stages alone can double the total carbon footprint of a project. Most studies that evaluate the environmental impact of language models (LLMs) only consider part of the problem. As some authors point out , it is essential to conduct a full life cycle assessment (LCA) . This means considering not only the energy used during training, but also the pollution linked to the manufacture of the required infrastructure (servers, storage devices, etc.), from the extraction of raw materials to end-of-life disposal. This includes two often overlooked components: Embodied emissions: all the pollution generated upstream (manufacturing, transport, installation of equipment); Idle consumption: electricity used by servers even when not actively in use. As an example, the carbon footprint associated with training the Bloom model rises from 25 to nearly 50 tonnes of CO\u2082 when all these parameters are included. Furthermore, it is important not to limit impact assessments to carbon emissions alone . One must consider all forms of pollution generated. For instance, the manufacture of chips used to train and run models requires vast quantities of water \u2014 another major ecological cost that is often overlooked. To assess the environmental impact of deep learning models, one must also take into account the energy spent on fine-tuning the models and running them during inference . While hard to estimate, some studies suggest that inference (using the model) accounts for the vast majority of energy consumption in large models (especially GPT-type).","title":"The energy consumption of models"},{"location":"environment/#a-few-additional-remarks","text":"At first glance, using models like BERT \u2014 whether for training, fine-tuning, or inference \u2014 appears less energy-intensive than using large generative models like GPT. However, even if these models consume relatively less, this is still important to keep in mind their potential impacts , especially if the number of users increases. As we have seen, current impact assessment methodologies do not account for all pollution sources . It is therefore essential to minimize unnecessary energy use and consistently question the relevance and necessity of each project in light of the energy it consumes . Furthermore, even in countries with a largely decarbonized energy mix, electricity remains a limited resource \u2014 one that could be allocated to other equally or more essential needs. A low-carbon mix does not justify irresponsible use, nor should it lead to forgetting the principle of digital sobriety .","title":"A few additional remarks"},{"location":"faq/","text":"FAQ How can I recover my password ? For the moment, the only solution is to contact the administrator of your service to reinitialize it for you. Can I launch processes if the GPU is already full ? You need to wait for enough GPU memory to be able to launch your process. There is no queue system for the moment. You can try to decrease the batch size to lower the required memory. If I destroyed my project, is it possible to recover the data ? No. I have bugs or repetitive problems Please open a issue on the Github. I have just created a project but want to modify certain parameters (e.g., increase the trainset size). Go to the project tab, navigate to Settings, and select Update Project. Please note that if you increase the size of your project, you then need to create a new feature in the Prepare tab.","title":"FAQ"},{"location":"faq/#faq","text":"","title":"FAQ"},{"location":"faq/#how-can-i-recover-my-password","text":"For the moment, the only solution is to contact the administrator of your service to reinitialize it for you.","title":"How can I recover my password ?"},{"location":"faq/#can-i-launch-processes-if-the-gpu-is-already-full","text":"You need to wait for enough GPU memory to be able to launch your process. There is no queue system for the moment. You can try to decrease the batch size to lower the required memory.","title":"Can I launch processes if the GPU is already full ?"},{"location":"faq/#if-i-destroyed-my-project-is-it-possible-to-recover-the-data","text":"No.","title":"If I destroyed my project, is it possible to recover the data ?"},{"location":"faq/#i-have-bugs-or-repetitive-problems","text":"Please open a issue on the Github.","title":"I have bugs or repetitive problems"},{"location":"faq/#i-have-just-created-a-project-but-want-to-modify-certain-parameters-eg-increase-the-trainset-size","text":"Go to the project tab, navigate to Settings, and select Update Project. Please note that if you increase the size of your project, you then need to create a new feature in the Prepare tab.","title":"I have just created a project but want to modify certain parameters (e.g., increase the trainset size)."},{"location":"quickstart/","text":"ActiveTigger Quickstart This is how to get started with your annotation project using ActiveTigger. ActiveTigger is a tool that will be useful if you want to: Quickly annotate a text-based dataset in a dedicated collaborative interface Train a model on a small set of annotated data to extend it on a larger corpus This guide explains the basics of all functionalities, but you do not have to use all of them for your project. For example, if you only want to use ActiveTigger for manual annotation, focus on the sections detailing how to set up and export your project data. ActiveTigger is in beta ActiveTigger is still in its beta version, and we are working on improving it. If you encounter any issues (or you have suggestions), please let us know on Github issues Note: ActiveTigger currently only works for multiclass/multilabel annotation, not span annotation. Table of contents Creating a project Workflow Project Tag Fine-tune Export User management Account 9. Storage 10. Further resources Creating a project The first step is to create a project. Creating the project is the most important step, since it will define the framework of your annotation process. First, you need to import raw data : a csv , xlsx or parquet file with your texts separated at the level you wish to annotate (sentences, paragraphs, social media posts, articles...). Each element should be a row. These will be loaded as the elements that you can individually annotate. It is up to you to produce this file. Give your project a name (the project name ). Each name is unique in the application and will allow you to identify your project. Name and id can be transformed by the process Both the project name and the ID will be transformed to be url-compatible. This means for instance that accentuated characters will be replaced by their non-accentuated equivalent, and spaces/underscores will be replaced by a dash. Do anticipate if you need to match this data further in the process with other informations. Specify the name of the column that contains the unique (numerical or textual) IDs for each element ( id columns ), and the name of the column (or columns) that contains the text ( text(s) columns ). Specify the language of the texts (used to priorize models and options in feature computation). If the file has already been annotated and you want to import these annotations, you can specify the column of existing annotations . Optionally, if there are context elements that you want to display while annotating (for example the author, the date, the publication...), you can specify the relevant columns here. You can view this information when annotating by going to Display Parameters and clicking on Contextual Information. The next step is to define both the training dataset that you will annotate and an optional test dataset that can be used for model evaluation. The training dataset is the most important since it will constitute the elements that you will be able to see and annotate. The underlying idea is that all computation will be limited to this dataset during the first phases of your project. You need to specify the number of elements you want in each dataset. Those elements will be then picked randomly in the raw data , prioritizing elements that have already been annotated if any for the training dataset . Using a test set is not mandatory. If you would like to validate your model on a test set, this will also be possible at a later stage further down the line. It is possible to stratify the elements of the test set, ensuring that each category of a given variable is equally represented. For example, if annotating a press corpus, you can ensure that the proportion of articles from Newspaper A and Newspaper B remains the same, even if their overall distribution in the corpus is uneven. Size of the dataset Once the project is created, you can start working on it. Visibility By default, a project is only visible to the user who created it (and the administrator of the service). You can add users on a project if you want to work collaboratively. Workflow Once you have created your project, below are the suggested steps that will help you develop, evaluate, and apply your model: Define your labels in your coding scheme Tag at least a couple hundred (randomly selected) elements (at least a few dozen per label) Train a Quick Model: define features under Project , train Quick Model under Tag Go back and annotate, alternating between Active Learning and random modes to ensure a diverse pool of annotated data in your training dataset. Check performance metrics (scores, visualizations) periodically to assess your annotation strategy. Once content with preliminary prediction performance, train a full BERT model under Fine-tune . You may need to go back and revise your annotation strategy if you are not happy with model performance. Once content with your fine-tuned model, apply it on a test set (unseen data) to evaluate it more robustly. Once content with these results, apply your model on the full dataset. Export the results and proceed with your analysis. In this guide, we go into more detail on each of these steps, going through the Active Tigger interface tab by tab. Project Click on the name of your project in the left-hand menu to see a summary of your where your project is at. Each project can have several coding schemes . A scheme is a set of specific labels that you can use to annotate the corpus. Each scheme works as an independant layer of annotation. One is created by default when you create a project. You can create a new coding scheme or delete an old one in the menu at the top. Creating a new coding scheme means starting from zero, but will not modify previous coding schemes. You can toggle between schemes as you go. Coding schemes There are two different coding schemes : multi-class and multi-label (the latter is still in experimental phase). Multi-class means one label per element; multi-label means several labels per element. You cannot switch between them, and multi-label are for the moment not completely implemented in the interface, so we recommand to test if the features you need are available. You can also see a summary of all your current annotations (per category), a history of all your actions in the project, and a summary of the parameters you set up while creating your project. You can also delete the project in the Parameters tab once you finished with it. Destroy a project Be aware that deleting a project will delete all your annotations and the project itself. This does also release space for other projects, so deleting projects you do not need is good practice. Once you entered the annotation phase, you will have an history of already annotated elements. This is the session history. Session history Be aware that you can only see any particular element once during a unique session, so if you need to re-annotate them, you will need to clear the history first. Prepare your project Schemes Before annotating, you need to define your labels. You do this by creating labels in a coding scheme. We recommend keeping your labels quick. If you are aiming to train a model, binary categorizations tend to be easier to handle. For example, if you are annotating newspaper headlines, it is easier to classify it as \"politics/not politics\", rather than to include all possible subjects as multiple categories. You can layer different binary categorizations as different coding scheme, or add labels at a later stage. Enter the name of each label under \"New label\" and click the plus sign. You can also delete or replace labels. If you want to delete a label, pick the relevant label under Available labels and then the trash bin. All existing annotations will be deleted. If you want to replace a label, pick the relevant label under Available labels , write the label's new name, and click the sign next to Replace selected label . All the existing annotations will be converted to the new label. Merging labels If you want to merge 2 labels in one, you just have to rename one with the name of the other. Features The Project tab also lets you define the features you want to use for your model. Features means that each text element is represented by a numerical vector. This is necessary to train certain models (especially for active learning) or to do projections. By default, we recommend using the SBERT feature, which is a pre-trained model that converts your text into a numerical representation. This is a good starting point for most projects. Codebook Under the Codebook tab, you can also include written instructions on how to distinguish your categories. It is helpful if you work collaboratively. Import Here you can import: - additional annotations to your train set (make sure they have the same id) - a test set to validate your model (more on this below) Parameters Here you can see (or change, if needed) a summary of the main parameters for your project. Session history This tab logs all activities undertaken on your project (can be particularly helpful if you work with collaborators). Tag The Tag tab is where you will spend most of your time. Here is where you annotate your data and perform your first measures to create and validate a model. Selection mode In the Annotate section, the interface will pick out an element that you can annotate according to your pre-defined labels. Once you have validated an annotation, the interface will pick the next element for you following the selection mode that is configured. The ID of the element is displayed in the URL. By default, the selection modes \"deterministic\" and \"random\" are available: Deterministic mode means that ActiveTigger will pick out each element in the order of the database, as created when creating your project. Random mode means that ActiveTigger will pick out the next element at random. Click on Get element if you want to apply a new selection mode after modifying it. The selection mode refers both the general rule of getting new elements (e.g. random) and specific rules, such as specified regular expressions ( regex ) patterns. You can search for elements with particular keywords or particular syntax patterns (regex). This could mean fishing out all elements that contain certain symbols, for example. If you are unfamiliar with regex patterns, this generator can be a useful reference. Keyboard shortcuts You can use the keyboard shortcuts to annotate faster. The number keys correspond to the labels you have defined. You can move the labels to change the order if needed. Add comment You can add a comment to each annotation. This can be useful to explain why you chose a certain label, or to note any particularities about the text. You can also : - go back to the previous annotated element (for instance, to re-annotate it) - skip an element in the current session (it will not be annotated, and you will need to clean the session to be able to see it again) Active learning Active learning is a method to accelerate the process of annotation and to improve dataset for model fine-tuning. Often, we want to classify imbalanced datasets, i.e. where one category is much less represented in the data than the other. This can mean very lengthy annotation processes, if you go through each element in a random order hoping to stumble upon both of your categories. Using the already annotated data, ActiveTigger can find the elements that your current model is either most certain or most uncertain that it knows how to predict, given your existing coding scheme and annotations. Here is how to set it up: First, make sure you have at least one feature created under the Prepare tab (by default, we recommend SBERT). Second, you need to select features under the Project tab and train a Quick Model Once the quick prediction model is trained, you can now choose the active and maxprob selection modes when picking elements. That means you can use the prediction of this model to guide your selection. Active mode means that Active Tigger will pick the elements on which it is most uncertain (where, based on previous annotations, it could be classified either way) Maxprob mode means that Active Tigger will pick the elements on which it is most certain (where, based on previous annotations, the model guesses where to categorize it with the highest levels of confidence). When constructing your training dataset, we recommend starting in random mode in order to create a base of annotations on which to train a prediction model. There is no absolute minimum number. A couple dozen annotations representing both of your labels can serve as a base. By default, you need to retrain the model at will, but you can also configure it to be retrained every N steps. If your aim is to train a model, we recommend alternating between active and maxprob mode in order to maximize the number of examples from both of your categories prioritizing on the uncertain elements. If displayed (see Display parameters), the Prediction button above your available labels indicates the model's prediction of a certain label (given previous annotations) and its level of certainty (you can deactivate it in Display parameters ) You can see, by clicking on active model , if your model works well on your data. The Exact score is calculated on all the data you have annotated. The model uses this same data for training and then evaluating its performance. It is normal for this score to be very high (close to 1.0), because the model has already seen all this data during training. For the CV10 score , the model automatically divides all your annotations into 10 equal parts and trains on 9 parts that represent 90% of the annotations. It does not take into account the remaining part, which corresponds to 10% of the annotations, during training. It then uses this set-aside part to test whether the model can correctly predict these annotations that it has not seen during training. Then it repeats the following operation 10 times. Quick Model Once you have annotated a sufficient number of examples (ideally a few hundred in total, at least around 50 per class/category) you can begin training classification models to assist and scale your annotation work. ActiveTigger supports a progressive workflow for model training. Training Quick Models to Evaluate Intermediary Performance You can train one or more \"quick\", intermediary models to see how well your model is performing based on your existing annotations. These can provide preliminary insights on baseline performance. This can help you in early stages of annotation, identifying cases where a model may struggle on ambiguities or other inconsistencies in your coding scheme/data. They will also permit you to use the Active Learning features. Define your features under Prepare tab (we recommend SBERT). This is a pre-trained model that converts your text into a numerical representation (embeddings). This also allows you to use the Visualization feature to see how this preliminary model groups texts. Under Create a new quick model , you can set up the basic parameters for your quick model. We recommend keeping them at default for your first try. Optionally, you can tick the box to include a 10-fold cross validation . This means that your quick model will \"test itself\" on your training data, 10 times. This gives you a first indication of your model's performance, but since you are using only training data, this should be taken with a grain of salt. Once you are done, you will see a series of metrics, of which the F1 score is generally the most interesting. See explanation of metrics under the Fine-tune section below. Later, once you have trained a full BERT model (see Fine-tune section) you can also use this model instead of a quick model to make predictions. You can the predictions of your latest (quick or no) model in visualizations, exporting predictions, and further annotation (active learning). Once you feel that you may have a large enough set of annotations and your quick model shows signs of understanding your coding scheme, you can then train a larger model (see Fine-tune section) with more parameters that you can modify, which can serve to obtain even more precise annotations. Keep your coding scheme consistent It can be tempting to adjust your annotations according to initial performance metrics (for example, a lower F1 score in a certain category may incite you to tag more examples as positives for this category). Ensure that your annotations are first and foremost consistent with your codebook. Mixing annotation strategies in the middle of the same scheme can cause issues with training models due to inconsistencies. If you are unhappy with model results, consider adjusting your codebook - maybe your categories are too vaguely defined? You can start a new coding scheme to test various annotation strategies. Tabular This tab gives you an overview of all tagged elements. You can get a quick picture of their distribution in the dataset. If you want to relabel an element, you can do so by double-clicking on the assigned label. Vizualisation To improve your coding, you can use the Visualization mode. Select your SBERT feature (you can also add your trained model if you have one), then click compute. This will project your models into two dimensions. This projection allows you to see how your models group similar texts naturally. You can either visualize the elements you have annotated or the model\u2019s predictions. The separation (or overlap) of the different groups can give you a visual indication of your model's performance \u2013 clear separation suggests a model that distinguishes categories well. Points appearing in \"incorrect regions\" (e.g., isolated orange points in a mostly blue area) may indicate classification errors or interesting edge cases to analyze. When you spot such cases, you can click on a point and annotate it. Fine-tune Active Tigger allows you to adapt a existing BERT classifier model on your annotated data. Once you did that, you can extend your annotation pattern on the complete dataset, or download this classifier for other uses. Basically, this is a process of fine-tuning: the pre-trained base model will be adjusted to your specific data. Click on Create to train a new model. Name it and pick which BERT model base you would like to use (note that some are language-specific, by default ModernBert in English and CamemBert in French). You can adjust the parameters for the model, or leave it at default values. Leave some time for the training process (you can follow the progress). Depending the parameters it will consume more or less computational power, especially GPU. It can take some time depending on the number of elements. Once the model is available, you can consult it under the Models tab. GPU load When available, the process will use GPU. Since resources are limited, overloads can happen. Consequently, a process can fail if there is no enough memory. You can follow the current state of the GPU use in the left menu of the screen. Fine-tuned models Once your model is trained, it will be selectable under this tab. At this point, you can evaluate its performance to decide to apply it on your data (train or test sets) to see metrics of its performance, or to extend it on the whole initial dataset. Choose the name of the model under Existing models , click on the Scores tab , and click Predict using train set . it will use the model on the training dataset (so on the elements you haven't annotated yet). Once the prediction is done, you will see a series of scores that allows you to evaluate the model's performance.: F1 micro : The harmonic mean of precision and recall, calculated globally without considering category imbalance. F1 macro : The harmonic mean of precision and recall calculated per class, treating all categories equally regardless of their size. F1 weighted : The harmonic mean of precision and recall calculated per class, weighted by the number of true instances in each category to account for imbalance. F1 : The harmonic mean of precision and recall (shown per each label) Precision : Proportion of correctly predicted positive cases out of all predicted positives. Recall : Proportion of correctly predicted positive cases out of all actual positives. Accuracy : Proportion of correctly classified elements out of total elements. All of these variables tell you useful information about how your model performs, but the way you assess them depends on your research question. For example, say that you are classifying social media posts according to whether they express support for climate policies or not. A low precision score means many posts labeled as \"supportive\" are actually irrelevant or against climate change policies (false positives). A low recall means the model misses many supportive posts (false negatives). Improving precision might involve stricter rules for classifying posts as supportive (e.g., requiring multiple positive keywords). However, this could hurt recall, as subtle supportive posts might be overlooked. The generic F1 score is often the variable most of interest, as it indicate how precision and recall are balanced. The closer the F1 score is to 1, the better the model performs according to the coding scheme you have trained it on. Active Tigger also provides a confusion matrix . You can read it as follows (for binary classifications: the first number represents the true negatives, meaning the documents correctly classified as belonging to the negative class. The second number represents the false positives (documents incorrectly classified as belonging to the positive class). The third number corresponds to the false negatives (documents incorrectly classified as not belonging to the positive class). Finally, the fourth number represents the true positives (documents correctly classified as belonging to the positive class). If you find yourself with low scores, it is a good idea to first consider your coding scheme. Are your categories clear? Several rounds of iterative annotations are often necessary as you refine your approach. If you realize that you have annotated certain words ambiguously, you can revise your coding from the Explore tab. To improve your score, you can also check in the False Prediction tab for texts where discrepancies between your coding and the model's coding is indicated. Try to understand the reason for the difference and revise your coding accordingly. Monitoring the Training Process During model training, ActiveTigger displays loss curves for both training and evaluation datasets. These curves help you assess the model's learning progress. The overall goal should be to minimize loss : a number that tells us how far off the model's predictions are from the correct answers. It is the difference between the model's predictions and the actual labeled data. CLick on Predict on train set to see the loss curves during training. During the traning phase, we calculate loss on training data. Training loss shows how well the model is doing on the examples it has already seen. Evaluation loss is measured on a separate sample of data that was not used during training. This gives a more realistic idea of how the model will perform on new, unseen data. This will give you two curves. They should be interpreted as follows: - Decreasing Loss: Indicates effective learning. If the loss remains flat, consider increasing the learning rate. - Increasing Loss: Suggests the learning rate may be too high, try lowering it. - Chaotic Loss Patterns: May indicate the model is learning from noise; adjust the learning rate accordingly. Overfitting: If the evaluation loss is significantly higher than the training loss, the model may be overfitting (= the model \"memorizes\" rather than \"learns\" and will perform poorly on unseen data). - Stalled Learning: If both curves are flat, consider reducing the number of epochs. You can also adjust the batch size (= how many examples the model looks at before updating itself) and gradient accumulation (= simulating large-batch training). The product of these two settings gives the effective batch size, which is displayed in the interface. These evaluation tools can help you define the parameters for your final model to be applied on the whole dataset. Another way to improve your coding is by going to the Visualization tab (under the Annotate tab), which helps you identify ambiguous cases. Once you find the model satisfactory, you can apply it to the whole dataset in the tab Compute prediction . This will apply the model to all the elements in the dataset, and you can then export the results. Test If you have defined or imported a test set, you can also apply the model on it. This is useful to see how the model performs on unseen data. It is seen as good practice to validate a model on a dedicated test set. The first step is to import the test set, then to select \"test set mode\". You should then go the the annotation tab. Once you have annotated enough data, you can calculate your scores to see if your model is robust. Predict To extend the annotations across all your data, select a model that you have trained and click on \u00ab Launch prediction complete dataset \u00bb. The file (which you can export from the Export tab) will then provide the probability associated with each text in your dataset. You can also apply your model on data that hasn't been uploaded on the project with \u00ab Import external texts to predict \u00bb. Export You can export your total annotations in csv , xlsx or parquet format. On the Export tab, select the desired format and click Export training data . You can also export the features and models you have trained if you wish to use them elsewhere. User management You can add users to your project. This is useful if you want to work collaboratively on a project. Create user The right to create users is currently restricted. Account You can change your password. Storage Please make sure to regularly delete unused models and archive projects no longer in active use, as these will take up unnecessary space. Further resources Interpreting Loss Curves \u2013 Google ML Crash Course (in French) A Deep Dive Into Learning Curves in Machine Learning \u2013 Weights & Biases","title":"Quickstart"},{"location":"quickstart/#activetigger-quickstart","text":"This is how to get started with your annotation project using ActiveTigger. ActiveTigger is a tool that will be useful if you want to: Quickly annotate a text-based dataset in a dedicated collaborative interface Train a model on a small set of annotated data to extend it on a larger corpus This guide explains the basics of all functionalities, but you do not have to use all of them for your project. For example, if you only want to use ActiveTigger for manual annotation, focus on the sections detailing how to set up and export your project data. ActiveTigger is in beta ActiveTigger is still in its beta version, and we are working on improving it. If you encounter any issues (or you have suggestions), please let us know on Github issues Note: ActiveTigger currently only works for multiclass/multilabel annotation, not span annotation.","title":"ActiveTigger Quickstart"},{"location":"quickstart/#table-of-contents","text":"Creating a project Workflow Project Tag Fine-tune Export User management Account 9. Storage 10. Further resources","title":"Table of contents"},{"location":"quickstart/#creating-a-project","text":"The first step is to create a project. Creating the project is the most important step, since it will define the framework of your annotation process. First, you need to import raw data : a csv , xlsx or parquet file with your texts separated at the level you wish to annotate (sentences, paragraphs, social media posts, articles...). Each element should be a row. These will be loaded as the elements that you can individually annotate. It is up to you to produce this file. Give your project a name (the project name ). Each name is unique in the application and will allow you to identify your project. Name and id can be transformed by the process Both the project name and the ID will be transformed to be url-compatible. This means for instance that accentuated characters will be replaced by their non-accentuated equivalent, and spaces/underscores will be replaced by a dash. Do anticipate if you need to match this data further in the process with other informations. Specify the name of the column that contains the unique (numerical or textual) IDs for each element ( id columns ), and the name of the column (or columns) that contains the text ( text(s) columns ). Specify the language of the texts (used to priorize models and options in feature computation). If the file has already been annotated and you want to import these annotations, you can specify the column of existing annotations . Optionally, if there are context elements that you want to display while annotating (for example the author, the date, the publication...), you can specify the relevant columns here. You can view this information when annotating by going to Display Parameters and clicking on Contextual Information. The next step is to define both the training dataset that you will annotate and an optional test dataset that can be used for model evaluation. The training dataset is the most important since it will constitute the elements that you will be able to see and annotate. The underlying idea is that all computation will be limited to this dataset during the first phases of your project. You need to specify the number of elements you want in each dataset. Those elements will be then picked randomly in the raw data , prioritizing elements that have already been annotated if any for the training dataset . Using a test set is not mandatory. If you would like to validate your model on a test set, this will also be possible at a later stage further down the line. It is possible to stratify the elements of the test set, ensuring that each category of a given variable is equally represented. For example, if annotating a press corpus, you can ensure that the proportion of articles from Newspaper A and Newspaper B remains the same, even if their overall distribution in the corpus is uneven. Size of the dataset Once the project is created, you can start working on it. Visibility By default, a project is only visible to the user who created it (and the administrator of the service). You can add users on a project if you want to work collaboratively.","title":"Creating a project"},{"location":"quickstart/#workflow","text":"Once you have created your project, below are the suggested steps that will help you develop, evaluate, and apply your model: Define your labels in your coding scheme Tag at least a couple hundred (randomly selected) elements (at least a few dozen per label) Train a Quick Model: define features under Project , train Quick Model under Tag Go back and annotate, alternating between Active Learning and random modes to ensure a diverse pool of annotated data in your training dataset. Check performance metrics (scores, visualizations) periodically to assess your annotation strategy. Once content with preliminary prediction performance, train a full BERT model under Fine-tune . You may need to go back and revise your annotation strategy if you are not happy with model performance. Once content with your fine-tuned model, apply it on a test set (unseen data) to evaluate it more robustly. Once content with these results, apply your model on the full dataset. Export the results and proceed with your analysis. In this guide, we go into more detail on each of these steps, going through the Active Tigger interface tab by tab.","title":"Workflow"},{"location":"quickstart/#project","text":"Click on the name of your project in the left-hand menu to see a summary of your where your project is at. Each project can have several coding schemes . A scheme is a set of specific labels that you can use to annotate the corpus. Each scheme works as an independant layer of annotation. One is created by default when you create a project. You can create a new coding scheme or delete an old one in the menu at the top. Creating a new coding scheme means starting from zero, but will not modify previous coding schemes. You can toggle between schemes as you go. Coding schemes There are two different coding schemes : multi-class and multi-label (the latter is still in experimental phase). Multi-class means one label per element; multi-label means several labels per element. You cannot switch between them, and multi-label are for the moment not completely implemented in the interface, so we recommand to test if the features you need are available. You can also see a summary of all your current annotations (per category), a history of all your actions in the project, and a summary of the parameters you set up while creating your project. You can also delete the project in the Parameters tab once you finished with it. Destroy a project Be aware that deleting a project will delete all your annotations and the project itself. This does also release space for other projects, so deleting projects you do not need is good practice. Once you entered the annotation phase, you will have an history of already annotated elements. This is the session history. Session history Be aware that you can only see any particular element once during a unique session, so if you need to re-annotate them, you will need to clear the history first.","title":"Project"},{"location":"quickstart/#prepare-your-project","text":"","title":"Prepare your project"},{"location":"quickstart/#schemes","text":"Before annotating, you need to define your labels. You do this by creating labels in a coding scheme. We recommend keeping your labels quick. If you are aiming to train a model, binary categorizations tend to be easier to handle. For example, if you are annotating newspaper headlines, it is easier to classify it as \"politics/not politics\", rather than to include all possible subjects as multiple categories. You can layer different binary categorizations as different coding scheme, or add labels at a later stage. Enter the name of each label under \"New label\" and click the plus sign. You can also delete or replace labels. If you want to delete a label, pick the relevant label under Available labels and then the trash bin. All existing annotations will be deleted. If you want to replace a label, pick the relevant label under Available labels , write the label's new name, and click the sign next to Replace selected label . All the existing annotations will be converted to the new label. Merging labels If you want to merge 2 labels in one, you just have to rename one with the name of the other.","title":"Schemes"},{"location":"quickstart/#features","text":"The Project tab also lets you define the features you want to use for your model. Features means that each text element is represented by a numerical vector. This is necessary to train certain models (especially for active learning) or to do projections. By default, we recommend using the SBERT feature, which is a pre-trained model that converts your text into a numerical representation. This is a good starting point for most projects.","title":"Features"},{"location":"quickstart/#codebook","text":"Under the Codebook tab, you can also include written instructions on how to distinguish your categories. It is helpful if you work collaboratively.","title":"Codebook"},{"location":"quickstart/#import","text":"Here you can import: - additional annotations to your train set (make sure they have the same id) - a test set to validate your model (more on this below)","title":"Import"},{"location":"quickstart/#parameters","text":"Here you can see (or change, if needed) a summary of the main parameters for your project.","title":"Parameters"},{"location":"quickstart/#session-history","text":"This tab logs all activities undertaken on your project (can be particularly helpful if you work with collaborators).","title":"Session history"},{"location":"quickstart/#tag","text":"The Tag tab is where you will spend most of your time. Here is where you annotate your data and perform your first measures to create and validate a model.","title":"Tag"},{"location":"quickstart/#selection-mode","text":"In the Annotate section, the interface will pick out an element that you can annotate according to your pre-defined labels. Once you have validated an annotation, the interface will pick the next element for you following the selection mode that is configured. The ID of the element is displayed in the URL. By default, the selection modes \"deterministic\" and \"random\" are available: Deterministic mode means that ActiveTigger will pick out each element in the order of the database, as created when creating your project. Random mode means that ActiveTigger will pick out the next element at random. Click on Get element if you want to apply a new selection mode after modifying it. The selection mode refers both the general rule of getting new elements (e.g. random) and specific rules, such as specified regular expressions ( regex ) patterns. You can search for elements with particular keywords or particular syntax patterns (regex). This could mean fishing out all elements that contain certain symbols, for example. If you are unfamiliar with regex patterns, this generator can be a useful reference. Keyboard shortcuts You can use the keyboard shortcuts to annotate faster. The number keys correspond to the labels you have defined. You can move the labels to change the order if needed. Add comment You can add a comment to each annotation. This can be useful to explain why you chose a certain label, or to note any particularities about the text. You can also : - go back to the previous annotated element (for instance, to re-annotate it) - skip an element in the current session (it will not be annotated, and you will need to clean the session to be able to see it again)","title":"Selection mode"},{"location":"quickstart/#active-learning","text":"Active learning is a method to accelerate the process of annotation and to improve dataset for model fine-tuning. Often, we want to classify imbalanced datasets, i.e. where one category is much less represented in the data than the other. This can mean very lengthy annotation processes, if you go through each element in a random order hoping to stumble upon both of your categories. Using the already annotated data, ActiveTigger can find the elements that your current model is either most certain or most uncertain that it knows how to predict, given your existing coding scheme and annotations. Here is how to set it up: First, make sure you have at least one feature created under the Prepare tab (by default, we recommend SBERT). Second, you need to select features under the Project tab and train a Quick Model Once the quick prediction model is trained, you can now choose the active and maxprob selection modes when picking elements. That means you can use the prediction of this model to guide your selection. Active mode means that Active Tigger will pick the elements on which it is most uncertain (where, based on previous annotations, it could be classified either way) Maxprob mode means that Active Tigger will pick the elements on which it is most certain (where, based on previous annotations, the model guesses where to categorize it with the highest levels of confidence). When constructing your training dataset, we recommend starting in random mode in order to create a base of annotations on which to train a prediction model. There is no absolute minimum number. A couple dozen annotations representing both of your labels can serve as a base. By default, you need to retrain the model at will, but you can also configure it to be retrained every N steps. If your aim is to train a model, we recommend alternating between active and maxprob mode in order to maximize the number of examples from both of your categories prioritizing on the uncertain elements. If displayed (see Display parameters), the Prediction button above your available labels indicates the model's prediction of a certain label (given previous annotations) and its level of certainty (you can deactivate it in Display parameters ) You can see, by clicking on active model , if your model works well on your data. The Exact score is calculated on all the data you have annotated. The model uses this same data for training and then evaluating its performance. It is normal for this score to be very high (close to 1.0), because the model has already seen all this data during training. For the CV10 score , the model automatically divides all your annotations into 10 equal parts and trains on 9 parts that represent 90% of the annotations. It does not take into account the remaining part, which corresponds to 10% of the annotations, during training. It then uses this set-aside part to test whether the model can correctly predict these annotations that it has not seen during training. Then it repeats the following operation 10 times.","title":"Active learning"},{"location":"quickstart/#quick-model","text":"Once you have annotated a sufficient number of examples (ideally a few hundred in total, at least around 50 per class/category) you can begin training classification models to assist and scale your annotation work. ActiveTigger supports a progressive workflow for model training. Training Quick Models to Evaluate Intermediary Performance You can train one or more \"quick\", intermediary models to see how well your model is performing based on your existing annotations. These can provide preliminary insights on baseline performance. This can help you in early stages of annotation, identifying cases where a model may struggle on ambiguities or other inconsistencies in your coding scheme/data. They will also permit you to use the Active Learning features. Define your features under Prepare tab (we recommend SBERT). This is a pre-trained model that converts your text into a numerical representation (embeddings). This also allows you to use the Visualization feature to see how this preliminary model groups texts. Under Create a new quick model , you can set up the basic parameters for your quick model. We recommend keeping them at default for your first try. Optionally, you can tick the box to include a 10-fold cross validation . This means that your quick model will \"test itself\" on your training data, 10 times. This gives you a first indication of your model's performance, but since you are using only training data, this should be taken with a grain of salt. Once you are done, you will see a series of metrics, of which the F1 score is generally the most interesting. See explanation of metrics under the Fine-tune section below. Later, once you have trained a full BERT model (see Fine-tune section) you can also use this model instead of a quick model to make predictions. You can the predictions of your latest (quick or no) model in visualizations, exporting predictions, and further annotation (active learning). Once you feel that you may have a large enough set of annotations and your quick model shows signs of understanding your coding scheme, you can then train a larger model (see Fine-tune section) with more parameters that you can modify, which can serve to obtain even more precise annotations. Keep your coding scheme consistent It can be tempting to adjust your annotations according to initial performance metrics (for example, a lower F1 score in a certain category may incite you to tag more examples as positives for this category). Ensure that your annotations are first and foremost consistent with your codebook. Mixing annotation strategies in the middle of the same scheme can cause issues with training models due to inconsistencies. If you are unhappy with model results, consider adjusting your codebook - maybe your categories are too vaguely defined? You can start a new coding scheme to test various annotation strategies.","title":"Quick Model"},{"location":"quickstart/#tabular","text":"This tab gives you an overview of all tagged elements. You can get a quick picture of their distribution in the dataset. If you want to relabel an element, you can do so by double-clicking on the assigned label.","title":"Tabular"},{"location":"quickstart/#vizualisation","text":"To improve your coding, you can use the Visualization mode. Select your SBERT feature (you can also add your trained model if you have one), then click compute. This will project your models into two dimensions. This projection allows you to see how your models group similar texts naturally. You can either visualize the elements you have annotated or the model\u2019s predictions. The separation (or overlap) of the different groups can give you a visual indication of your model's performance \u2013 clear separation suggests a model that distinguishes categories well. Points appearing in \"incorrect regions\" (e.g., isolated orange points in a mostly blue area) may indicate classification errors or interesting edge cases to analyze. When you spot such cases, you can click on a point and annotate it.","title":"Vizualisation"},{"location":"quickstart/#fine-tune","text":"Active Tigger allows you to adapt a existing BERT classifier model on your annotated data. Once you did that, you can extend your annotation pattern on the complete dataset, or download this classifier for other uses. Basically, this is a process of fine-tuning: the pre-trained base model will be adjusted to your specific data. Click on Create to train a new model. Name it and pick which BERT model base you would like to use (note that some are language-specific, by default ModernBert in English and CamemBert in French). You can adjust the parameters for the model, or leave it at default values. Leave some time for the training process (you can follow the progress). Depending the parameters it will consume more or less computational power, especially GPU. It can take some time depending on the number of elements. Once the model is available, you can consult it under the Models tab. GPU load When available, the process will use GPU. Since resources are limited, overloads can happen. Consequently, a process can fail if there is no enough memory. You can follow the current state of the GPU use in the left menu of the screen.","title":"Fine-tune"},{"location":"quickstart/#fine-tuned-models","text":"Once your model is trained, it will be selectable under this tab. At this point, you can evaluate its performance to decide to apply it on your data (train or test sets) to see metrics of its performance, or to extend it on the whole initial dataset. Choose the name of the model under Existing models , click on the Scores tab , and click Predict using train set . it will use the model on the training dataset (so on the elements you haven't annotated yet). Once the prediction is done, you will see a series of scores that allows you to evaluate the model's performance.: F1 micro : The harmonic mean of precision and recall, calculated globally without considering category imbalance. F1 macro : The harmonic mean of precision and recall calculated per class, treating all categories equally regardless of their size. F1 weighted : The harmonic mean of precision and recall calculated per class, weighted by the number of true instances in each category to account for imbalance. F1 : The harmonic mean of precision and recall (shown per each label) Precision : Proportion of correctly predicted positive cases out of all predicted positives. Recall : Proportion of correctly predicted positive cases out of all actual positives. Accuracy : Proportion of correctly classified elements out of total elements. All of these variables tell you useful information about how your model performs, but the way you assess them depends on your research question. For example, say that you are classifying social media posts according to whether they express support for climate policies or not. A low precision score means many posts labeled as \"supportive\" are actually irrelevant or against climate change policies (false positives). A low recall means the model misses many supportive posts (false negatives). Improving precision might involve stricter rules for classifying posts as supportive (e.g., requiring multiple positive keywords). However, this could hurt recall, as subtle supportive posts might be overlooked. The generic F1 score is often the variable most of interest, as it indicate how precision and recall are balanced. The closer the F1 score is to 1, the better the model performs according to the coding scheme you have trained it on. Active Tigger also provides a confusion matrix . You can read it as follows (for binary classifications: the first number represents the true negatives, meaning the documents correctly classified as belonging to the negative class. The second number represents the false positives (documents incorrectly classified as belonging to the positive class). The third number corresponds to the false negatives (documents incorrectly classified as not belonging to the positive class). Finally, the fourth number represents the true positives (documents correctly classified as belonging to the positive class). If you find yourself with low scores, it is a good idea to first consider your coding scheme. Are your categories clear? Several rounds of iterative annotations are often necessary as you refine your approach. If you realize that you have annotated certain words ambiguously, you can revise your coding from the Explore tab. To improve your score, you can also check in the False Prediction tab for texts where discrepancies between your coding and the model's coding is indicated. Try to understand the reason for the difference and revise your coding accordingly.","title":"Fine-tuned models"},{"location":"quickstart/#monitoring-the-training-process","text":"During model training, ActiveTigger displays loss curves for both training and evaluation datasets. These curves help you assess the model's learning progress. The overall goal should be to minimize loss : a number that tells us how far off the model's predictions are from the correct answers. It is the difference between the model's predictions and the actual labeled data. CLick on Predict on train set to see the loss curves during training. During the traning phase, we calculate loss on training data. Training loss shows how well the model is doing on the examples it has already seen. Evaluation loss is measured on a separate sample of data that was not used during training. This gives a more realistic idea of how the model will perform on new, unseen data. This will give you two curves. They should be interpreted as follows: - Decreasing Loss: Indicates effective learning. If the loss remains flat, consider increasing the learning rate. - Increasing Loss: Suggests the learning rate may be too high, try lowering it. - Chaotic Loss Patterns: May indicate the model is learning from noise; adjust the learning rate accordingly. Overfitting: If the evaluation loss is significantly higher than the training loss, the model may be overfitting (= the model \"memorizes\" rather than \"learns\" and will perform poorly on unseen data). - Stalled Learning: If both curves are flat, consider reducing the number of epochs. You can also adjust the batch size (= how many examples the model looks at before updating itself) and gradient accumulation (= simulating large-batch training). The product of these two settings gives the effective batch size, which is displayed in the interface. These evaluation tools can help you define the parameters for your final model to be applied on the whole dataset. Another way to improve your coding is by going to the Visualization tab (under the Annotate tab), which helps you identify ambiguous cases. Once you find the model satisfactory, you can apply it to the whole dataset in the tab Compute prediction . This will apply the model to all the elements in the dataset, and you can then export the results.","title":"Monitoring the Training Process"},{"location":"quickstart/#test","text":"If you have defined or imported a test set, you can also apply the model on it. This is useful to see how the model performs on unseen data. It is seen as good practice to validate a model on a dedicated test set. The first step is to import the test set, then to select \"test set mode\". You should then go the the annotation tab. Once you have annotated enough data, you can calculate your scores to see if your model is robust.","title":"Test"},{"location":"quickstart/#predict","text":"To extend the annotations across all your data, select a model that you have trained and click on \u00ab Launch prediction complete dataset \u00bb. The file (which you can export from the Export tab) will then provide the probability associated with each text in your dataset. You can also apply your model on data that hasn't been uploaded on the project with \u00ab Import external texts to predict \u00bb.","title":"Predict"},{"location":"quickstart/#export","text":"You can export your total annotations in csv , xlsx or parquet format. On the Export tab, select the desired format and click Export training data . You can also export the features and models you have trained if you wish to use them elsewhere.","title":"Export"},{"location":"quickstart/#user-management","text":"You can add users to your project. This is useful if you want to work collaboratively on a project. Create user The right to create users is currently restricted.","title":"User management"},{"location":"quickstart/#account","text":"You can change your password.","title":"Account"},{"location":"quickstart/#storage","text":"Please make sure to regularly delete unused models and archive projects no longer in active use, as these will take up unnecessary space.","title":"Storage"},{"location":"quickstart/#further-resources","text":"Interpreting Loss Curves \u2013 Google ML Crash Course (in French) A Deep Dive Into Learning Curves in Machine Learning \u2013 Weights & Biases","title":"Further resources"},{"location":"software/","text":"Software characteristics Roadmap Calendar Experimental generative for April. Stable classical version planned for mid-June (+ Docker). Next Steps Multilabel workflow + bert-fine tuning Create Python wrapper Write documentation + tutorial Optimize vizualisation for large dataset \u2699 Create a easy/medium/pro mode need definition Define the monitor panel need definition Optimize GPU management (prediction) Enhancements Build Docker image \u2699 Integrate genAI tools \u2699 Add carbon count Add new models (Modernbert, Phi3, pleias, ...) Refactor design (better ergonomy / colors / etc.) \u2699 Better data set management (expand) Animate community on Discord Possibilities Attribute specific task to users Architecture This is a collection of technical points/choices for the app Overall architecture : backend : Python/FastAPI frontend : React/Typescript Backend config.yaml define the parameters at the server launch The unit is the project, composes of different classes Features Schemes Quickmodels Bertmodels Users CPU/GPU bound computation is managed in separated processes with a queue State of the service is checked at each request (with a threshold) Data management Tabular data is stored as separated parquet files divided in train / test / complete SQLite database to manage annotations/parameters/users/logs Projects are loaded into memory to facilitate computation (filter, etc.) Unloaded after one day Bert models are saved in dedicated filesystems Processes ProcessPoolExecutor with workers https://superfastpython.com/processpoolexecutor-in-python/ Different type of parallel process : training ; predicting Only one process possible by user/project Users role Role-Based Access Control (RBAC) - 3 roles : root, manager, annotator Authentification with OAuth2 and token in header Table of valid tokens A table of authorization defines the relation users/projects Different uses can modify a same project : no lock Select element to annotate The selection combines different strategy : filters and/or active learning. Active learning is a prediction with a model trained on already annotated data. Different modes of selection deterministic aleatory maxprob for a label max entropy Pipeline of choice sample (tagged, untagged, all) regex proba / entropie Frontend State management Each project is described by its general state (not user specific) Computed/computing elements","title":"Software"},{"location":"software/#software-characteristics","text":"","title":"Software characteristics"},{"location":"software/#roadmap","text":"","title":"Roadmap"},{"location":"software/#calendar","text":"Experimental generative for April. Stable classical version planned for mid-June (+ Docker).","title":"Calendar"},{"location":"software/#next-steps","text":"Multilabel workflow + bert-fine tuning Create Python wrapper Write documentation + tutorial Optimize vizualisation for large dataset \u2699 Create a easy/medium/pro mode need definition Define the monitor panel need definition Optimize GPU management (prediction)","title":"Next Steps"},{"location":"software/#enhancements","text":"Build Docker image \u2699 Integrate genAI tools \u2699 Add carbon count Add new models (Modernbert, Phi3, pleias, ...) Refactor design (better ergonomy / colors / etc.) \u2699 Better data set management (expand) Animate community on Discord","title":"Enhancements"},{"location":"software/#possibilities","text":"Attribute specific task to users","title":"Possibilities"},{"location":"software/#architecture","text":"This is a collection of technical points/choices for the app Overall architecture : backend : Python/FastAPI frontend : React/Typescript","title":"Architecture"},{"location":"software/#backend","text":"config.yaml define the parameters at the server launch The unit is the project, composes of different classes Features Schemes Quickmodels Bertmodels Users CPU/GPU bound computation is managed in separated processes with a queue State of the service is checked at each request (with a threshold)","title":"Backend"},{"location":"software/#data-management","text":"Tabular data is stored as separated parquet files divided in train / test / complete SQLite database to manage annotations/parameters/users/logs Projects are loaded into memory to facilitate computation (filter, etc.) Unloaded after one day Bert models are saved in dedicated filesystems","title":"Data management"},{"location":"software/#processes","text":"ProcessPoolExecutor with workers https://superfastpython.com/processpoolexecutor-in-python/ Different type of parallel process : training ; predicting Only one process possible by user/project","title":"Processes"},{"location":"software/#users-role","text":"Role-Based Access Control (RBAC) - 3 roles : root, manager, annotator Authentification with OAuth2 and token in header Table of valid tokens A table of authorization defines the relation users/projects Different uses can modify a same project : no lock","title":"Users role"},{"location":"software/#select-element-to-annotate","text":"The selection combines different strategy : filters and/or active learning. Active learning is a prediction with a model trained on already annotated data. Different modes of selection deterministic aleatory maxprob for a label max entropy Pipeline of choice sample (tagged, untagged, all) regex proba / entropie","title":"Select element to annotate"},{"location":"software/#frontend","text":"","title":"Frontend"},{"location":"software/#state-management","text":"Each project is described by its general state (not user specific) Computed/computing elements","title":"State management"},{"location":"usecases/","text":"Use cases Jules Brion research Study Objective : To build a corpus of newspapers articles related to the environment and the ecological transition. Corpus : Articles are first extracted from Europresse with a list of keywords related to the planet boundaries. The problem is that many of these words (\"climate\", \"environment\"...) are polysemic. With a traditional approach, the corpus should then be built up manually, separating relevant articles from those not related to the environment. The problem is that this severely limits the number of articles you can add to your corpus, which can be a problem when analyzing media content on issues as broad as the environment. With active Tigger, it was possible to skip the initial corpus and sort through it. Codebook : Binary classification \u2013 environment / not environment. Goal : Develop a classifier with high performance (F1 score > 0.95) capable of identifying environmental journalism segments. Recomandations : Active Tigger sets a limit of 512 tokens (around 1300 characters), so it's not possible to analyze an entire article. Generally speaking, it is sometimes easier to classify a 300-character text than a 1300-character text. The decision was therefore taken to keep texts of less than 500 characters intact and to cut out other texts between 300 and 700 characters, with priority given to the end of a sentence (period, question mark, etc.). If the components of your corpus are not homogeneous (there are many more articles from one newspaper than another, for example), it can sometimes be useful to create an additional homogeneous corpus to train the model . In general, it's best if the training corpus reflects as closely as possible the diversity of cases that will be encountered by the classifier. For example, don't annotate only one year if you are then submitting a corpus from ten years ago. Once the model has been trained and stabilized, you can apply it to your \"heterogeneous corpus\". When training your model, it is a good practice to examine the cases where the model's predictions differ from your manual annotations. This can help you spot potential annotation mistakes due to inattention. However, be careful to code according to your codebook and not according to the model's predictions to avoid overfitting . Once your model has been trained and stabilized, you can apply it to corpora of any size. What is particularly useful about this approach is that, should your study corpus change (you want to add a year of study or add a newspaper, for example) you can still classify it (by going to Predict, then choose your model and go to Import external texts to predict ). Post-ActiveTigger Analysis : Once the annotations has been extended to the entire corpus, the article paragraphs are grouped together. A score is calculated, relating the number of environment-related paragraphs to the total number of paragraphs for each article. Depending on the needs of your search, it is possible to go a little further in post active-tigger processing. For example, to minimize the number of false positives (paragraphs considered to be related to the environment when they are not), it is possible to remove from the calculation paragraphs where the model is not sure of its prediction (refer to the enthropy score). In addition, we decided not to keep articles with three paragraphs and only one related to the environment (the score of 0.33 is high, but adding them runs the risk of false positives because it is only based on one classification). Key Takeaways / Lessons Learned : Learning ActiveTigger: It took a few weeks to get comfortable with the interface, stabilize the corpus structure (e.g., paragraph length), and define the coding scheme. Use of Visualization: Visualization tools helped identify ambiguous categories, leading to annotation improvements. Model Training Insights: Understanding how BERT-based models learn is essential for improving performance metrics . Initially, models were trained without a clear grasp of how they function or which parameters influence outcomes, which lead to common pitfalls such as overfitting. Taking the time to understand the fundamentals \u2014 like the distinction between evaluation loss and validation loss \u2014 helps in effectively avoiding these issues.","title":"Use Cases"},{"location":"usecases/#use-cases","text":"","title":"Use cases"},{"location":"usecases/#jules-brion-research","text":"Study Objective : To build a corpus of newspapers articles related to the environment and the ecological transition. Corpus : Articles are first extracted from Europresse with a list of keywords related to the planet boundaries. The problem is that many of these words (\"climate\", \"environment\"...) are polysemic. With a traditional approach, the corpus should then be built up manually, separating relevant articles from those not related to the environment. The problem is that this severely limits the number of articles you can add to your corpus, which can be a problem when analyzing media content on issues as broad as the environment. With active Tigger, it was possible to skip the initial corpus and sort through it. Codebook : Binary classification \u2013 environment / not environment. Goal : Develop a classifier with high performance (F1 score > 0.95) capable of identifying environmental journalism segments. Recomandations : Active Tigger sets a limit of 512 tokens (around 1300 characters), so it's not possible to analyze an entire article. Generally speaking, it is sometimes easier to classify a 300-character text than a 1300-character text. The decision was therefore taken to keep texts of less than 500 characters intact and to cut out other texts between 300 and 700 characters, with priority given to the end of a sentence (period, question mark, etc.). If the components of your corpus are not homogeneous (there are many more articles from one newspaper than another, for example), it can sometimes be useful to create an additional homogeneous corpus to train the model . In general, it's best if the training corpus reflects as closely as possible the diversity of cases that will be encountered by the classifier. For example, don't annotate only one year if you are then submitting a corpus from ten years ago. Once the model has been trained and stabilized, you can apply it to your \"heterogeneous corpus\". When training your model, it is a good practice to examine the cases where the model's predictions differ from your manual annotations. This can help you spot potential annotation mistakes due to inattention. However, be careful to code according to your codebook and not according to the model's predictions to avoid overfitting . Once your model has been trained and stabilized, you can apply it to corpora of any size. What is particularly useful about this approach is that, should your study corpus change (you want to add a year of study or add a newspaper, for example) you can still classify it (by going to Predict, then choose your model and go to Import external texts to predict ). Post-ActiveTigger Analysis : Once the annotations has been extended to the entire corpus, the article paragraphs are grouped together. A score is calculated, relating the number of environment-related paragraphs to the total number of paragraphs for each article. Depending on the needs of your search, it is possible to go a little further in post active-tigger processing. For example, to minimize the number of false positives (paragraphs considered to be related to the environment when they are not), it is possible to remove from the calculation paragraphs where the model is not sure of its prediction (refer to the enthropy score). In addition, we decided not to keep articles with three paragraphs and only one related to the environment (the score of 0.33 is high, but adding them runs the risk of false positives because it is only based on one classification). Key Takeaways / Lessons Learned : Learning ActiveTigger: It took a few weeks to get comfortable with the interface, stabilize the corpus structure (e.g., paragraph length), and define the coding scheme. Use of Visualization: Visualization tools helped identify ambiguous categories, leading to annotation improvements. Model Training Insights: Understanding how BERT-based models learn is essential for improving performance metrics . Initially, models were trained without a clear grasp of how they function or which parameters influence outcomes, which lead to common pitfalls such as overfitting. Taking the time to understand the fundamentals \u2014 like the distinction between evaluation loss and validation loss \u2014 helps in effectively avoiding these issues.","title":"Jules Brion research"}]}